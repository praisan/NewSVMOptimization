{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5/r4Q20tS+Zmv4zVtVlci"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xi8ptt_Unqjz"},"outputs":[],"source":["import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.base import TransformerMixin,BaseEstimator\n","class NewSVM(TransformerMixin,BaseEstimator):\n","\n","    def __init__(self,alpha0=0.4,a=0.6,r=100):\n","      self.alpha0=alpha0\n","      self.a=a\n","      self.r=r\n","\n","    def fit(self, Xtrain, Ytrain, verbose=True):\n","        Xtrain=Xtrain.T\n","        Ytrain=Ytrain.T[np.newaxis, :]\n","        d, m = Xtrain.shape\n","        print('Xtrain.shape',Xtrain.shape )\n","        print('Ytrain.shape',Ytrain.shape )\n","\n","\n","        # define hyperparameter\n","        geval_limit = 1e6;\n","        maxitr = int(geval_limit)\n","        rbox = 1e7\n","        taubar_arr = [0, 1, 2, 5, 10] # delay\n","        time_limit = 10;\n","        lambda1 = 1\n","        lambda2 = 0.5\n","\n","        # output variable\n","        maxidx = len(taubar_arr) + 2\n","        hinge = np.zeros((maxidx, maxitr))\n","        acc = np.zeros((maxidx, maxitr))\n","        geval = np.zeros((maxidx, maxitr))\n","        itr_flag = np.zeros((maxidx, time_limit))\n","        runtimes = np.zeros((len(taubar_arr)))\n","        results = pd.DataFrame({\"taubar\": taubar_arr, \"niter\": [0]*len(taubar_arr), \"hinge\": [0]*len(taubar_arr), \"acc\": [0]*len(taubar_arr), \"time\": [0]*len(taubar_arr), \"TP\": [0]*len(taubar_arr), \"TN\": [0]*len(taubar_arr), \"FP\": [0]*len(taubar_arr), \"FN\": [0]*len(taubar_arr)})\n","\n","        for idx in range(len(taubar_arr)):\n","\n","            tau_max = taubar_arr[idx]\n","            print(\"r = %d tau_bar = %d\\n\"%(self.r, tau_max))\n","\n","            grad = np.zeros((1, d))\n","            self.w = np.zeros((d, 1))\n","            grad_num = 0\n","\n","            st = time.time()\n","            # start the algorithm\n","            for k in range(maxitr + 1):\n","\n","                # runtime-related script\n","                runtime = time.time() - st\n","                if runtime > time_limit:\n","                    break\n","                # ****************************** #\n","                itr_flag[idx, int(np.ceil(runtime)-1)] = k\n","                # ****************************** #\n","\n","                alpha =  (self.alpha0/(k+1)) * ((1/(8+2*((tau_max+1)**2)))**(1/self.a))\n","                sumvec = np.zeros((d, 1))\n","                for i in range(m+2):\n","                    if (k % (tau_max + 1) == 0) | (tau_max == 0):\n","                        grad = np.zeros((1, d))\n","                        grad_num = grad_num + 1\n","\n","                        if i + 1 <= m: # subgradient of generalized hinge loss\n","                            if Ytrain[:,i].dot(Xtrain[:,i].dot(self.w)) <= 0:\n","                                grad = -(self.r/m) * Ytrain[:,i].dot(Xtrain[:,i].reshape(1,-1))\n","                            elif (Ytrain[:,i].dot(Xtrain[:,i].dot(self.w)) > 0) & (Ytrain[:,i].dot(Xtrain[:,i].dot(self.w)) < 1):\n","                                grad = -(1/m) * Ytrain[:,i].dot(Xtrain[:,i].reshape(1,-1))\n","                            else:\n","                                grad = np.zeros((1, d))\n","\n","                        elif i + 1 == m+1: # L1 norm:\n","                            for j in range(d):\n","                                if self.w[j] < 0:\n","                                    grad[:,j] = -1\n","                                elif self.w[j] > 0:\n","                                    grad[:,j] = 1\n","                                else:\n","                                    grad[:,j] = 0\n","                            grad = lambda1 * grad\n","\n","                        elif i + 1 == m+2: # L2 norm\n","                            grad = self.w.T.dot(lambda2)\n","\n","                    y_ik = self.w - alpha*grad.T.reshape(-1,1)\n","\n","                    # solve the subproblem\n","                    x_ik = np.minimum(np.maximum(y_ik, -rbox), rbox)\n","                    sumvec = sumvec + x_ik\n","\n","                w_next = sumvec/(m+2)\n","\n","                hinge[idx, k+1] = self.general_hinge_loss(Xtrain, Ytrain, self.r)/m\n","                acc[idx, k+1] = (self.classify(Xtrain)==Ytrain).sum()/m\n","                geval[idx, k+1] = grad_num\n","                self.w = w_next\n","            evaltime = time.time() - st\n","            runtimes[idx] = evaltime\n","            print(\" nitr = %d hinge = %.6f time = %.6f\\n\"%(k, hinge[idx, k],runtimes[idx]))\n","        return self\n","\n","    def general_hinge_loss(self,X, Y, r):\n","        \"\"\"\n","        Compute general hinge loss\n","\n","        Parameters\n","        ----------\n","        w : A d x 1 matrix\n","        X : A d x m matrix\n","        Y : A 1 x m matrix\n","        r : a constant\n","\n","        Returns\n","        -------\n","        sum_loss : A numeric value.\n","\n","        \"\"\"\n","        d, m = X.shape\n","        res = np.zeros((m,1))\n","\n","        for i in range(m):\n","            val = Y[:,i].dot(X[:,i].dot(self.w))\n","            if val <= 0:\n","                res[i] = (1 - r*val)\n","            elif (val < 1) & (val > 0):\n","                res[i] = (1 - val)\n","            elif val >= 1:\n","                res[i] = 0\n","\n","        sum_loss = sum(res)\n","        return sum_loss\n","\n","    def predict(self, X):\n","        return self.w.T@X.T\n","\n","    def classify(self,X):\n","        return np.where(self.predict(X.T) >= 0.0, 1, -1)"]}]}